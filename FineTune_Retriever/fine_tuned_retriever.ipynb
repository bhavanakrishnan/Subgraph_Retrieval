{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Set\n",
    "\n",
    "from utils import dump_jsonl, load_jsonl\n",
    "from tqdm import tqdm\n",
    "from func_timeout import func_set_timeout, FunctionTimedOut\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from knowledge_graph.knowledge_graph import KnowledgeGraph\n",
    "from knowledge_graph.knowledge_graph_cache import KnowledgeGraphCache\n",
    "from knowledge_graph.knowledge_graph_freebase import KnowledgeGraphFreebase\n",
    "\n",
    "from config import cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load model begin]\n"
     ]
    }
   ],
   "source": [
    "END_REL = \"END OF HOP\"\n",
    "\n",
    "TOP_K = 10\n",
    "\n",
    "retrieval_model_ckpt = cfg.retriever_model_ckpt\n",
    "device = 'cuda'\n",
    "\n",
    "print(\"[load model begin]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tmp/subgraph_hop1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kg \u001b[38;5;241m=\u001b[39m \u001b[43mKnowledgeGraphCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(retrieval_model_ckpt)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(retrieval_model_ckpt)\n",
      "File \u001b[0;32m~/ERIC_PROJECTS/SUBGRAPHS/My_Code/FineTune_Retriever/knowledge_graph/knowledge_graph_cache.py:11\u001b[0m, in \u001b[0;36mKnowledgeGraphCache.__init__\u001b[0;34m(self, kb_filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, kb_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/subgraph_hop1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriples: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m                        ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_triples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkb_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead2relation: Dict[\u001b[38;5;28mstr\u001b[39m, Set[\u001b[38;5;28mstr\u001b[39m]\n\u001b[1;32m     13\u001b[0m                              ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_head2relation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriples)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_relation2tail: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Set[\u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m     15\u001b[0m                                   ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_head_relation2tail(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriples)\n",
      "File \u001b[0;32m~/ERIC_PROJECTS/SUBGRAPHS/My_Code/FineTune_Retriever/knowledge_graph/knowledge_graph_cache.py:18\u001b[0m, in \u001b[0;36mKnowledgeGraphCache._load_triples\u001b[0;34m(self, kb_filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_triples\u001b[39m(\u001b[38;5;28mself\u001b[39m, kb_filename) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m---> 18\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkb_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     triples \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tmp/subgraph_hop1.txt'"
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraphCache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(retrieval_model_ckpt)\n",
    "model = AutoModel.from_pretrained(retrieval_model_ckpt)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"[load model end]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_subgraph(topic_entity: str, path: List[str]):\n",
    "    return kg.deduce_subgraph_by_path(topic_entity, path)\n",
    "\n",
    "def path_to_candidate_relations(topic_entity: str, path: List[str]) -> List[str]:\n",
    "    new_relations = kg.deduce_leaves_relation_by_path(topic_entity, path)\n",
    "    # filter relation\n",
    "    candidate_relations = [r for r in new_relations if r.split(\".\")[0] not in [\"kg\", \"common\"]]\n",
    "    \n",
    "    # limit = 10\n",
    "    # candidate_relations = [r for r in candidate_relations if \n",
    "    #                        kg.deduce_leaves_count_by_path(topic_entity, path + [r]) <= limit ** (len(path) + 1)]\n",
    "    \n",
    "    return list(candidate_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_texts_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True,\n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    embeddings = model(**inputs, output_hidden_states=True,\n",
    "                       return_dict=True).pooler_output\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_path_list_and_relation_list(question: str, path_list: List[List[str]], path_score_list: List[float], relation_list_list: List[List[str]], theta: float = 0.07) -> List[Tuple[List[str], float]]:\n",
    "    results = []\n",
    "    \n",
    "    query_lined_list = ['#'.join([question] + path) for path in path_list]\n",
    "    \n",
    "    # SR w/o QU # TMP\n",
    "    # query_lined_list = [question for _ in path_list]\n",
    "    \n",
    "    all_relation_list = list(set(sum(relation_list_list, [])))\n",
    "    # END_relation_index = all_relation_list.index(END_REL)\n",
    "    q_emb = get_texts_embeddings(query_lined_list).unsqueeze(1)  # [B, 1, D]\n",
    "    target_emb = get_texts_embeddings(all_relation_list).unsqueeze(0)  # [1, L, D]\n",
    "    sim_score = torch.cosine_similarity(q_emb, target_emb, dim=2) / theta  # [B, L]\n",
    "    # end_score = sim_score[:, END_relation_index].unsqueeze(1)  # [B, 1]\n",
    "    # sigmoid_score = torch.sigmoid((sim_score))  # 1 / (1 + exp(-(f(r) - f(end))))\n",
    "    for i, (path, path_score, relation_list) in enumerate(zip(path_list, path_score_list, relation_list_list)):\n",
    "        for relation in relation_list:\n",
    "            j = all_relation_list.index(relation)\n",
    "            new_path = path + [relation]\n",
    "            score = float(sim_score[i, j]) + path_score\n",
    "            results.append((new_path, score))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def infer_paths_from_kb(question: str, topic_entity: str, num_beams: int, num_return_paths: int, max_hop: int) -> List[Tuple[List[str], float]]:\n",
    "    \"\"\"从KB中进行宽为num_beams的搜索,得到num_return_paths条路径并提供对应得分\"\"\"\n",
    "    candidate_paths = [[[], 0]]  # path and its score\n",
    "    result_paths = []\n",
    "    n_hop = 0\n",
    "    while candidate_paths and len(result_paths) < num_return_paths and n_hop < max_hop:\n",
    "        search_list = []\n",
    "        # try every possible next_hop\n",
    "        relation_list_list = []\n",
    "        path_list = []\n",
    "        path_score_list = []\n",
    "        # logger.info(f'candidate_paths: {candidate_paths}')\n",
    "        for path, path_score in candidate_paths:\n",
    "            path_list.append(path)\n",
    "            path_score_list.append(path_score)\n",
    "            # logger.info(f'path_to_candidate_relations: {topic_entity}, {path}')\n",
    "            candidate_relations = path_to_candidate_relations(\n",
    "                topic_entity, path)\n",
    "            candidate_relations = candidate_relations + [END_REL]\n",
    "            relation_list_list.append(candidate_relations)\n",
    "        # logger.info(f'path_list and relation_list_list:{path_list} {relation_list_list}')\n",
    "        search_list = score_path_list_and_relation_list(\n",
    "            question, path_list, path_score_list, relation_list_list)\n",
    "\n",
    "        search_list = sorted(search_list, key=lambda x: x[1], reverse=True)[\n",
    "            :num_beams]\n",
    "        # Update candidate_paths and result_paths\n",
    "        n_hop = n_hop + 1\n",
    "        candidate_paths = []\n",
    "        for path, score in search_list:\n",
    "            if path[-1] == END_REL:\n",
    "                result_paths.append([path, score])\n",
    "            else:\n",
    "                candidate_paths.append([path, score])\n",
    "    # Force early stop\n",
    "    if n_hop == max_hop and candidate_paths:\n",
    "        for path, score in candidate_paths:\n",
    "            path = path + [END_REL]\n",
    "            result_paths.append([path, score])\n",
    "    result_paths = sorted(result_paths, key=lambda x: x[1], reverse=True)[\n",
    "        :num_return_paths]\n",
    "    return result_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reverse_graph(G:Dict[str, List[str]]):\n",
    "    r_G:Dict[str,List[str]] = dict()\n",
    "    for u in G:\n",
    "        for v in G[u]:\n",
    "            r_G.setdefault(v, []).append(u)\n",
    "    return r_G\n",
    "\n",
    "def bfs_graph(G:Dict[str, List[str]],root):\n",
    "    \"\"\"\n",
    "    G: a adjacency list in Dict\n",
    "    return: all bfs nodes\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    currentLevel = [root]\n",
    "    while currentLevel:\n",
    "        for v in currentLevel:\n",
    "            visited.add(v)\n",
    "        nextLevel = set()\n",
    "        # levelGraph = {v:set() for v in currentLevel}\n",
    "        for v in currentLevel:\n",
    "            for w in G.get(v,[]):\n",
    "                if w not in visited:\n",
    "                    # levelGraph[v].add(w)\n",
    "                    nextLevel.add(w)\n",
    "        # yield levelGraph\n",
    "        currentLevel = nextLevel\n",
    "    return visited\n",
    "\n",
    "\n",
    "def merge_graph(graph_l, root_l, graph_r, root_r):\n",
    "    assert root_l != root_r\n",
    "    all_nodes = set()\n",
    "    common_nodes = set(graph_l) & set(graph_r)\n",
    "    all_nodes |= common_nodes\n",
    "    reverse_graph_l, reverse_graph_r = _reverse_graph(graph_l), _reverse_graph(graph_r)\n",
    "    for node in common_nodes:\n",
    "        ancestors_l = bfs_graph(reverse_graph_l, node)\n",
    "        ancestors_r = bfs_graph(reverse_graph_r, node)\n",
    "        descendants_l = bfs_graph(graph_l, node)\n",
    "        descendants_r = bfs_graph(graph_r, node)\n",
    "        all_nodes.update(ancestors_l)\n",
    "        all_nodes.update(ancestors_r)\n",
    "        all_nodes.update(descendants_l)\n",
    "        all_nodes.update(descendants_r)\n",
    "    return all_nodes\n",
    "\n",
    "\n",
    "def filter_by_graph(nodes: List[str], triples: List[str], G: Dict[str, List[str]]):\n",
    "    entities = set(G.keys())\n",
    "    nodes = [e for e in nodes if e in entities]\n",
    "    triples = [(h, r, t) for h, r, t in triples if h in entities and t in entities]\n",
    "    return nodes, triples\n",
    "\n",
    "\n",
    "def build_graph(nodes: List[str], triples: List[str]):\n",
    "    G = {}\n",
    "    for e in nodes:\n",
    "        G[e] = []\n",
    "    for h, _, t in triples:\n",
    "        G.setdefault(h, []).append(t)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_subgraph(json_obj: Dict[str, Any], entities: Set, rels: Set):\n",
    "    data_list = []\n",
    "\n",
    "    question = json_obj[\"question\"]\n",
    "    if len(json_obj[\"entities\"]) == 0:\n",
    "        return\n",
    "    \n",
    "    answers = set([ans_obj[\"kb_id\"] for ans_obj in json_obj[\"answers\"]])\n",
    "        \n",
    "    for topic_entity in json_obj[\"entities\"]:\n",
    "\n",
    "        path_score_list = infer_paths_from_kb(question, topic_entity, TOP_K, TOP_K, 2)\n",
    "        nodes = []\n",
    "        triples = []\n",
    "\n",
    "        min_score = 1e5\n",
    "    \n",
    "        threshold_ent_size = 1000\n",
    "        for path, score in path_score_list:\n",
    "            partial_nodes, partial_triples = path_to_subgraph(topic_entity, path)\n",
    "            if len(partial_nodes) > 1000:\n",
    "                continue\n",
    "            \n",
    "            partial_nodes = [e for e in partial_nodes if e in entities]\n",
    "            partial_triples = [(h, r, t) for h, r, t in partial_triples \n",
    "                               if h in entities and t in entities and r in rels]\n",
    "            \n",
    "            nodes.extend(partial_nodes)\n",
    "            triples.extend(partial_triples)\n",
    "            \n",
    "            data_list.append({\n",
    "                \"id\": json_obj[\"id\"],\n",
    "                \"question\": json_obj[\"question\"],\n",
    "                \"entities\": json_obj[\"entities\"],\n",
    "                \"answers\": json_obj[\"answers\"],\n",
    "                \"paths\": [(topic_entity, path)],\n",
    "                \"subgraph\": {\n",
    "                    \"entities\": partial_nodes,\n",
    "                    \"tuples\": partial_triples\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            if len(answers & set(partial_nodes)) > 0:\n",
    "                min_score = min(min_score, score)\n",
    "            if len(nodes) > threshold_ent_size:\n",
    "                break\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_entities(load_data_path):\n",
    "    entities = []\n",
    "    with open(os.path.join(load_data_path, \"entities.txt\"), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            entities.append(line.strip())\n",
    "    return entities\n",
    "\n",
    "def build_relations(load_data_path):\n",
    "    rels = []\n",
    "    with open(os.path.join(load_data_path, \"relations.txt\"), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            rels.append(line.strip())\n",
    "    return rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieve_subgraph():\n",
    "    load_data_folder = cfg.retriever_finetune[\"load_data_path\"]\n",
    "    \n",
    "    train_dataset = load_jsonl(os.path.join(load_data_folder, \"train_simple.json\"))\n",
    "\n",
    "    entities = build_entities(load_data_folder)\n",
    "    rels = build_relations(load_data_folder)\n",
    "    \n",
    "    entities = set(entities)\n",
    "    rels = set(rels)\n",
    "    \n",
    "    new_dataset = []\n",
    "    for json_obj in tqdm(train_dataset, desc=\"retrieve:train\"):\n",
    "        data_list = retrieve_subgraph(json_obj, entities, rels)\n",
    "        new_dataset.extend(data_list)\n",
    "    \n",
    "    dump_jsonl(new_dataset, os.path.join(load_data_folder, \"finetune_simple.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    num_epoch = 1\n",
    "    for _ in range(num_epoch):\n",
    "        load_data_path = cfg.retriever_finetune[\"load_data_path\"]\n",
    "        dump_model_path = cfg.retriever_finetune[\"dump_model_path\"]\n",
    "        checkpoint_dir = cfg.retriever_finetune[\"checkpoint_dir\"]\n",
    "        dump_data_path = cfg.retriever_finetune[\"dump_data_path\"]\n",
    "        \n",
    "        if not os.path.exists(dump_model_path):\n",
    "            os.makedirs(dump_model_path)\n",
    "        \n",
    "        # step0\n",
    "        run_retrieve_subgraph()\n",
    "        # step1\n",
    "        subprocess.run([\"bash\", \"run_retriever_finetune.sh\", load_data_path, dump_model_path, checkpoint_dir, dump_data_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eric_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
