{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "from typing import Tuple, List, Any, Dict\n",
    "\n",
    "from utils import dump_jsonl, load_jsonl\n",
    "from tqdm import tqdm\n",
    "from func_timeout import func_set_timeout, FunctionTimedOut\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from knowledge_graph.knowledge_graph import KnowledgeGraph\n",
    "from knowledge_graph.knowledge_graph_cache import KnowledgeGraphCache\n",
    "from knowledge_graph.knowledge_graph_freebase import KnowledgeGraphFreebase\n",
    "from config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "END_REL = \"END OF HOP\"\n",
    "\n",
    "TOP_K = 10\n",
    "\n",
    "_min_score = 1e5\n",
    "\n",
    "retrieval_model_ckpt = \"\"\n",
    "device = 'cuda'\n",
    "\n",
    "print(\"[load model begin]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = KnowledgeGraphCache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(retrieval_model_ckpt)\n",
    "model = AutoModel.from_pretrained(retrieval_model_ckpt)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"[load model end]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_subgraph(topic_entity: str, path: List[str]):\n",
    "    return kg.deduce_subgraph_by_path(topic_entity, path)\n",
    "\n",
    "def path_to_candidate_relations(topic_entity: str, path: List[str]) -> List[str]:\n",
    "    new_relations = kg.deduce_leaves_relation_by_path(topic_entity, path)\n",
    "    # filter relation\n",
    "    candidate_relations = [r for r in new_relations if r.split(\".\")[0] not in [\"kg\", \"common\"]]\n",
    "    \n",
    "    # limit = 10\n",
    "    # candidate_relations = [r for r in candidate_relations if \n",
    "    #                        kg.deduce_leaves_count_by_path(topic_entity, path + [r]) <= limit ** (len(path) + 1)]\n",
    "    \n",
    "    return list(candidate_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_texts_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True,\n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    embeddings = model(**inputs, output_hidden_states=True,\n",
    "                       return_dict=True).pooler_output\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_path_list_and_relation_list(question: str, path_list: List[List[str]], path_score_list: List[float], relation_list_list: List[List[str]], theta: float = 0.07) -> List[Tuple[List[str], float]]:\n",
    "    results = []\n",
    "    \n",
    "    query_lined_list = ['#'.join([question] + path) for path in path_list]\n",
    "    \n",
    "    # SR w/o QU # TMP\n",
    "    # query_lined_list = [question for _ in path_list]\n",
    "    \n",
    "    all_relation_list = list(set(sum(relation_list_list, [])))\n",
    "    # END_relation_index = all_relation_list.index(END_REL)\n",
    "    q_emb = get_texts_embeddings(query_lined_list).unsqueeze(1)  # [B, 1, D]\n",
    "    target_emb = get_texts_embeddings(all_relation_list).unsqueeze(0)  # [1, L, D]\n",
    "    sim_score = torch.cosine_similarity(q_emb, target_emb, dim=2) / theta  # [B, L]\n",
    "    # end_score = sim_score[:, END_relation_index].unsqueeze(1)  # [B, 1]\n",
    "    # sigmoid_score = torch.sigmoid((sim_score))  # 1 / (1 + exp(-(f(r) - f(end))))\n",
    "    for i, (path, path_score, relation_list) in enumerate(zip(path_list, path_score_list, relation_list_list)):\n",
    "        for relation in relation_list:\n",
    "            j = all_relation_list.index(relation)\n",
    "            new_path = path + [relation]\n",
    "            score = float(sim_score[i, j]) + path_score\n",
    "            results.append((new_path, score))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_paths_from_kb(question: str, topic_entity: str, num_beams: int, num_return_paths: int, max_hop: int) -> List[Tuple[List[str], float]]:\n",
    "    candidate_paths = [[[], 0]]  # path and its score\n",
    "    result_paths = []\n",
    "    n_hop = 0\n",
    "    while candidate_paths and len(result_paths) < num_return_paths and n_hop < max_hop:\n",
    "        search_list = []\n",
    "        # try every possible next_hop\n",
    "        relation_list_list = []\n",
    "        path_list = []\n",
    "        path_score_list = []\n",
    "        # logger.info(f'candidate_paths: {candidate_paths}')\n",
    "        for path, path_score in candidate_paths:\n",
    "            path_list.append(path)\n",
    "            path_score_list.append(path_score)\n",
    "            # logger.info(f'path_to_candidate_relations: {topic_entity}, {path}')\n",
    "            candidate_relations = path_to_candidate_relations(\n",
    "                topic_entity, path)\n",
    "            candidate_relations = candidate_relations + [END_REL]\n",
    "            relation_list_list.append(candidate_relations)\n",
    "        # logger.info(f'path_list and relation_list_list:{path_list} {relation_list_list}')\n",
    "        search_list = score_path_list_and_relation_list(\n",
    "            question, path_list, path_score_list, relation_list_list)\n",
    "\n",
    "        search_list = sorted(search_list, key=lambda x: x[1], reverse=True)[\n",
    "            :num_beams]\n",
    "        # Update candidate_paths and result_paths\n",
    "        n_hop = n_hop + 1\n",
    "        candidate_paths = []\n",
    "        for path, score in search_list:\n",
    "            if path[-1] == END_REL:\n",
    "                result_paths.append([path, score])\n",
    "            else:\n",
    "                candidate_paths.append([path, score])\n",
    "    # Force early stop\n",
    "    if n_hop == max_hop and candidate_paths:\n",
    "        for path, score in candidate_paths:\n",
    "            path = path + [END_REL]\n",
    "            result_paths.append([path, score])\n",
    "    result_paths = sorted(result_paths, key=lambda x: x[1], reverse=True)[\n",
    "        :num_return_paths]\n",
    "    return result_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reverse_graph(G:Dict[str, List[str]]):\n",
    "    r_G:Dict[str,List[str]] = dict()\n",
    "    for u in G:\n",
    "        for v in G[u]:\n",
    "            r_G.setdefault(v, []).append(u)\n",
    "    return r_G\n",
    "\n",
    "def bfs_graph(G:Dict[str, List[str]],root):\n",
    "    \"\"\"\n",
    "    G: a adjacency list in Dict\n",
    "    return: all bfs nodes\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    currentLevel = [root]\n",
    "    while currentLevel:\n",
    "        for v in currentLevel:\n",
    "            visited.add(v)\n",
    "        nextLevel = set()\n",
    "        # levelGraph = {v:set() for v in currentLevel}\n",
    "        for v in currentLevel:\n",
    "            for w in G.get(v,[]):\n",
    "                if w not in visited:\n",
    "                    # levelGraph[v].add(w)\n",
    "                    nextLevel.add(w)\n",
    "        # yield levelGraph\n",
    "        currentLevel = nextLevel\n",
    "    return visited\n",
    "\n",
    "\n",
    "def merge_graph(graph_l, root_l, graph_r, root_r):\n",
    "    assert root_l != root_r\n",
    "    all_nodes = set()\n",
    "    common_nodes = set(graph_l) & set(graph_r)\n",
    "    all_nodes |= common_nodes\n",
    "    reverse_graph_l, reverse_graph_r = _reverse_graph(graph_l), _reverse_graph(graph_r)\n",
    "    for node in common_nodes:\n",
    "        ancestors_l = bfs_graph(reverse_graph_l, node)\n",
    "        ancestors_r = bfs_graph(reverse_graph_r, node)\n",
    "        descendants_l = bfs_graph(graph_l, node)\n",
    "        descendants_r = bfs_graph(graph_r, node)\n",
    "        all_nodes.update(ancestors_l)\n",
    "        all_nodes.update(ancestors_r)\n",
    "        all_nodes.update(descendants_l)\n",
    "        all_nodes.update(descendants_r)\n",
    "    return all_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_graph(nodes: List[str], triples: List[str], G: Dict[str, List[str]]):\n",
    "    entities = set(G.keys())\n",
    "    nodes = [e for e in nodes if e in entities]\n",
    "    triples = [(h, r, t) for h, r, t in triples if h in entities and t in entities]\n",
    "    return nodes, triples\n",
    "\n",
    "\n",
    "def build_graph(nodes: List[str], triples: List[str]):\n",
    "    G = {}\n",
    "    for e in nodes:\n",
    "        G[e] = []\n",
    "    for h, _, t in triples:\n",
    "        G.setdefault(h, []).append(t)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_subgraph(json_obj: Dict[str, Any], entities):\n",
    "    # logger.info(\"[sample]\")\n",
    "\n",
    "    question = json_obj[\"question\"]\n",
    "    if len(json_obj[\"entities\"]) == 0:\n",
    "        return\n",
    "    \n",
    "    answers = set([ans_obj[\"kb_id\"] for ans_obj in json_obj[\"answers\"]])\n",
    "    \n",
    "    paths = []  # List[Tuple[str, List[relation]]]\n",
    "    graphs = []\n",
    "    \n",
    "    # print(\"len entities:\", len(json_obj[\"entities\"]))\n",
    "    # logger.info(f'question: {question}')\n",
    "    for entity_id in json_obj[\"entities\"]:\n",
    "        topic_entity = entities[entity_id]\n",
    "\n",
    "        path_score_list = infer_paths_from_kb(question, topic_entity, TOP_K, TOP_K, 2)\n",
    "        nodes = []\n",
    "        triples = []\n",
    "\n",
    "        min_score = 1e5\n",
    "    \n",
    "        threshold_ent_size = 1000\n",
    "        for path, score in path_score_list:\n",
    "            partial_nodes, partial_triples = path_to_subgraph(topic_entity, path)\n",
    "            if len(partial_nodes) > 1000:\n",
    "                continue\n",
    "            paths.append((topic_entity, path))\n",
    "            nodes.extend(partial_nodes)\n",
    "            triples.extend(partial_triples)\n",
    "            \n",
    "            if len(answers & set(partial_nodes)) > 0:\n",
    "                min_score = min(min_score, score)\n",
    "            if len(nodes) > threshold_ent_size:\n",
    "                break\n",
    "        graphs.append((topic_entity, nodes, triples, build_graph(nodes, triples)))\n",
    "    \n",
    "    \n",
    "    n = len(graphs)\n",
    "    for i in range(1, n):\n",
    "        g0 = graphs[0]\n",
    "        gi = graphs[i]\n",
    "        topic_entity = g0[0]\n",
    "        nodes = merge_graph(g0[3], g0[0], gi[3], gi[0])\n",
    "        triples = [(h, r, t) for h, r, t in list(set(g0[2]) | set(gi[2])) if h in nodes and t in nodes]        \n",
    "        graph = build_graph(nodes, triples)\n",
    "        graphs[0] = (topic_entity, nodes, triples, graph)\n",
    "    \n",
    "    nodes = graphs[0][1]\n",
    "    triples = graphs[0][2]\n",
    "\n",
    "    global _min_score\n",
    "    _min_score = min(_min_score, min_score)\n",
    "    \n",
    "    nodes = list(set(nodes))\n",
    "    triples = list(set(triples))\n",
    "    subgraph_entities = [e for e in nodes]\n",
    "    subgraph_tuples = [(h, r, t) for h, r, t in triples]\n",
    "    json_obj[\"paths\"] = paths\n",
    "    json_obj[\"entities\"] = [entities[e] for e in json_obj[\"entities\"]]\n",
    "    json_obj[\"subgraph\"] = {\n",
    "        \"tuples\": subgraph_tuples,\n",
    "        \"entities\": subgraph_entities\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_entities(load_data_path):\n",
    "    entities = []\n",
    "    with open(os.path.join(load_data_path, \"entities.txt\"), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            entities.append(line.strip())\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieve_subgraph():\n",
    "    load_data_folder = cfg.retrieve_subgraph[\"load_data_folder\"]\n",
    "    dump_data_folder = cfg.retrieve_subgraph[\"dump_data_folder\"]\n",
    "    \n",
    "    if not os.path.exists(dump_data_folder):\n",
    "        os.makedirs(dump_data_folder)\n",
    "\n",
    "    \n",
    "    subprocess.run([\"cp\", \"-r\", load_data_folder, os.path.dirname(dump_data_folder)])\n",
    "    \n",
    "    train_dataset = load_jsonl(os.path.join(load_data_folder, \"train_simple.json\"))\n",
    "    test_dataset = load_jsonl(os.path.join(load_data_folder, \"test_simple.json\"))    \n",
    "    dev_dataset = load_jsonl(os.path.join(load_data_folder, \"dev_simple.json\"))    \n",
    "\n",
    "    entities = build_entities(load_data_folder)\n",
    "    \n",
    "    for json_obj in tqdm(train_dataset, desc=\"retrieve:train\"):\n",
    "        retrieve_subgraph(json_obj, entities)\n",
    "    \n",
    "    for json_obj in tqdm(test_dataset, desc=\"retrieve:test\"):\n",
    "        retrieve_subgraph(json_obj, entities)\n",
    "\n",
    "    for json_obj in tqdm(dev_dataset, desc=\"retrieve:dev\"):\n",
    "        retrieve_subgraph(json_obj, entities)\n",
    "\n",
    "    dump_jsonl(train_dataset, os.path.join(dump_data_folder, \"train_simple.json\"))\n",
    "    dump_jsonl(test_dataset, os.path.join(dump_data_folder, \"test_simple.json\"))\n",
    "    dump_jsonl(dev_dataset, os.path.join(dump_data_folder, \"dev_simple.json\"))\n",
    "\n",
    "    print(\"min score:\", _min_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_build_relation_set():\n",
    "    load_data_path = cfg.retrieve_subgraph[\"dump_data_folder\"]\n",
    "\n",
    "    train_dataset = load_jsonl(os.path.join(load_data_path, \"train_simple.json\"))\n",
    "    test_dataset = load_jsonl(os.path.join(load_data_path, \"test_simple.json\"))\n",
    "    dev_dataset = load_jsonl(os.path.join(load_data_path, \"dev_simple.json\"))\n",
    "\n",
    "    entity_set = set()\n",
    "    relation_set = set()\n",
    "    out_entity_set_filename = os.path.join(load_data_path, 'entities.txt')\n",
    "    out_relation_set_filename = os.path.join(load_data_path, 'relations.txt')\n",
    "\n",
    "    for dataset in [train_dataset, test_dataset, dev_dataset]:\n",
    "        for json_obj in tqdm(dataset):\n",
    "            answers = {ans_json_obj[\"kb_id\"]\n",
    "                    for ans_json_obj in json_obj[\"answers\"]}\n",
    "            subgraph_entities = set(json_obj[\"subgraph\"][\"entities\"]) | set(json_obj[\"entities\"])\n",
    "            subgraph_relations = {r for h, r, t in json_obj[\"subgraph\"][\"tuples\"]}\n",
    "            entity_set = entity_set | answers | subgraph_entities\n",
    "            relation_set = relation_set | subgraph_relations\n",
    "\n",
    "    def dump_list_to_txt(mylist, outname):\n",
    "        with open(outname, 'w') as f:\n",
    "            for item in mylist:\n",
    "                print(item, file=f)\n",
    "\n",
    "    entity_set = sorted(entity_set)\n",
    "    relation_set = sorted(relation_set)\n",
    "\n",
    "    dump_list_to_txt(entity_set, out_entity_set_filename)\n",
    "    dump_list_to_txt(relation_set, out_relation_set_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_retrieve_subgraph()\n",
    "run_build_relation_set()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
