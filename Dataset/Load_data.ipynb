{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import glob\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "import math\n",
    "import networkx as nx\n",
    "import os\n",
    "import random\n",
    "from tkinter import ALL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from func_timeout import func_set_timeout, FunctionTimedOut\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_REL = \"END OF HOP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_jsonl, dump_jsonl\n",
    "from knowledge_graph.knowledge_graph import KnowledgeGraph\n",
    "from knowledge_graph.knowledge_graph_cache import KnowledgeGraphCache\n",
    "from knowledge_graph.knowledge_graph_freebase import KnowledgeGraphFreebase\n",
    "from config import cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WebQSP\n",
    "def load_webqsp():\n",
    "    load_data_path = cfg.preprocessing[\"step0\"][\"load_data_path\"]\n",
    "    dump_data_path = cfg.preprocessing[\"step0\"][\"dump_data_path\"]\n",
    "    folder_path = cfg.preprocessing[\"step0\"][\"dump_data_folder\"]\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    with open(load_data_path, \"r\") as f:\n",
    "        train_dataset = json.loads(f.read())\n",
    "        data_list = []\n",
    "        for json_obj in train_dataset[\"Questions\"]:\n",
    "            question = json_obj[\"ProcessedQuestion\"]\n",
    "            for parse in json_obj[\"Parses\"]:\n",
    "                topic_entities = [parse[\"TopicEntityMid\"]]\n",
    "                answers = []\n",
    "                for answer_json_obj in parse[\"Answers\"]:\n",
    "                    if answer_json_obj[\"AnswerType\"] == \"Entity\":\n",
    "                        answers.append(answer_json_obj[\"AnswerArgument\"])\n",
    "                if len(answers) == 0:\n",
    "                    continue\n",
    "                data_list.append({\n",
    "                    \"question\": question,\n",
    "                    \"topic_entities\": topic_entities,\n",
    "                    \"answers\": answers,\n",
    "                })\n",
    "        with open(dump_data_path, \"w\") as f:\n",
    "            for json_obj in data_list:\n",
    "                f.write(json.dumps(json_obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_set_timeout(10)\n",
    "def generate_paths(item, kg: KnowledgeGraphFreebase, pair_max: int = 20, path_max: int = 100):\n",
    "    paths = []\n",
    "    entities = [entity for entity in item['topic_entities']]\n",
    "    answers = [answer for answer in item['answers']]\n",
    "    for src in entities:\n",
    "        for tgt in answers:\n",
    "            if len(paths) > path_max:\n",
    "                break\n",
    "            n_paths = []\n",
    "            n_paths.extend(kg.search_one_hop_relaiotn(src, tgt))\n",
    "            n_paths.extend(kg.search_two_hop_relation(src, tgt))\n",
    "            paths.extend(n_paths)\n",
    "    return paths[:path_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search_to_get_path():\n",
    "    load_data_path = cfg.preprocessing[\"step1\"][\"load_data_path\"]\n",
    "    dump_data_path = cfg.preprocessing[\"step1\"][\"dump_data_path\"]\n",
    "    kg = KnowledgeGraphFreebase()\n",
    "    train_dataset = load_jsonl(load_data_path)\n",
    "    \n",
    "    outf = open(dump_data_path, 'w')\n",
    "    for item in tqdm(train_dataset):\n",
    "        try:\n",
    "            paths = generate_paths(item, kg)\n",
    "        except FunctionTimedOut:\n",
    "            continue\n",
    "        outline = json.dumps([item, paths], ensure_ascii=False)\n",
    "        print(outline, file=outf)\n",
    "        outf.flush()\n",
    "    outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_score_path():\n",
    "    load_data_path = cfg.preprocessing[\"step2\"][\"load_data_path\"]\n",
    "    dump_data_path = cfg.preprocessing[\"step2\"][\"dump_data_path\"]\n",
    "\n",
    "    kg = KnowledgeGraphFreebase()\n",
    "\n",
    "    data_list = load_jsonl(load_data_path)\n",
    "\n",
    "    data_with_path_list = []\n",
    "    for (item, paths) in tqdm(data_list):\n",
    "        m = set()\n",
    "        for path in paths:\n",
    "            if isinstance(path, str):\n",
    "                path = (path,)\n",
    "            path = tuple(path)\n",
    "            if path == (\"type.object.type\", \"type.type.instance\"):\n",
    "                continue\n",
    "            m.add(path)\n",
    "        data_with_path_list.append((item, tuple(m)))\n",
    "\n",
    "    def cal_path_val(topic_entity, path, answers):\n",
    "        preds = kg.deduce_leaves_by_path(topic_entity, path)\n",
    "        preds = set(preds)\n",
    "        hit = preds & answers\n",
    "        full = preds\n",
    "        if not full:\n",
    "            return 1\n",
    "        return len(hit) / len(full)\n",
    "\n",
    "    m_list = []\n",
    "    for item, p_strs in tqdm(data_with_path_list):\n",
    "        answers = set(item['answers'])\n",
    "        topic_entities = item['topic_entities']\n",
    "        path_and_score_list = []\n",
    "        for p_str in p_strs:\n",
    "            path = p_str\n",
    "            p_val_list = []\n",
    "            for topic_entity in topic_entities:\n",
    "                p_val_list.append(cal_path_val(topic_entity, path, answers))\n",
    "            p_val = max(p_val_list)\n",
    "            path_and_score_list.append(dict(path=path, score=p_val))\n",
    "        m_item = deepcopy(item)\n",
    "        m_item['path_and_score_list'] = path_and_score_list\n",
    "        m_list.append(m_item)\n",
    "    \n",
    "    dump_jsonl(m_list, dump_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@func_set_timeout(10)\n",
    "def generate_data_list(path_json_obj, json_obj, pos_rels, kg: KnowledgeGraphFreebase):\n",
    "    new_data_list = []\n",
    "    neg_num = 15\n",
    "\n",
    "    path = path_json_obj[\"path\"]\n",
    "    path = path + [END_REL]\n",
    "    question = json_obj[\"question\"] + \" [SEP]\"\n",
    "    topic_entities = json_obj[\"topic_entities\"]\n",
    "    \n",
    "    filter_threshold = 5\n",
    "    current_filter_threshold = 1\n",
    "    filter_flag = False\n",
    "    \n",
    "\n",
    "    for rel in path[:-1]:\n",
    "        current_filter_threshold *= filter_threshold\n",
    "        \n",
    "        candidate_entities = set()\n",
    "        for h in topic_entities:\n",
    "            candidate_entities |= set(kg.get_hr2t_with_limit(h, rel, current_filter_threshold + 1))\n",
    "            if len(candidate_entities) > current_filter_threshold:\n",
    "                break\n",
    "\n",
    "        if len(candidate_entities) > current_filter_threshold:\n",
    "            filter_flag = True\n",
    "            break\n",
    "    \n",
    "    if filter_flag:\n",
    "        return None\n",
    "\n",
    "    prefix_list = []\n",
    "    for rel in path:\n",
    "        prefix = \",\".join(prefix_list)\n",
    "        prefix_list.append(rel)\n",
    "        \n",
    "        data_row = []\n",
    "        data_row.append(question)\n",
    "        data_row.append(rel)\n",
    "\n",
    "        neg_rels = set()\n",
    "        for h in topic_entities:\n",
    "            neg_rels |= set(kg.get_relation(h, limit=100))\n",
    "            if len(neg_rels) > 100:\n",
    "                break\n",
    "        neg_rels = list(neg_rels)\n",
    "        neg_rels.append(END_REL)\n",
    "        neg_rels = [r for r in neg_rels if r not in pos_rels[prefix]]\n",
    "\n",
    "        if len(neg_rels) > 0:\n",
    "            sample_rels = []\n",
    "            while len(sample_rels) < neg_num:\n",
    "                sample_rels.extend(neg_rels)\n",
    "            \n",
    "            neg_rels = random.sample(sample_rels, neg_num)            \n",
    "            neg_rels = neg_rels[:neg_num]\n",
    "            \n",
    "            data_row.extend(neg_rels)\n",
    "            new_data_list.append(data_row)\n",
    "        \n",
    "        # update for next step\n",
    "        if rel != END_REL:\n",
    "            next_question = question + f\" {rel} #\"\n",
    "            question = next_question\n",
    "            topic_entities = kg.deduce_leaves_from_src_list_and_relation(topic_entities, rel)\n",
    "\n",
    "    return new_data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_negative_sampling():\n",
    "    load_data_path = cfg.preprocessing[\"step3\"][\"load_data_path\"]\n",
    "    dump_data_path = cfg.preprocessing[\"step3\"][\"dump_data_path\"]\n",
    "    folder_path = cfg.preprocessing[\"step3\"][\"dump_data_folder\"]\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    kg = KnowledgeGraphFreebase()\n",
    "    threshold = 0.5\n",
    "    \n",
    "    data_list = load_jsonl(load_data_path)\n",
    "    update_paths = {}\n",
    "\n",
    "    new_data_list = []\n",
    "    timeout_count = 0\n",
    "    for json_obj in tqdm(data_list, desc=\"negative-sampling\"):\n",
    "        path_and_score_list = json_obj[\"path_and_score_list\"]\n",
    "        path_and_score_list = [path_json_obj for path_json_obj in path_and_score_list if path_json_obj[\"score\"] >= threshold]\n",
    "        pos_rels = {}  # 1-hop positive, 2-hop positive, ...\n",
    "        for path_json_obj in path_and_score_list:\n",
    "            path = path_json_obj[\"path\"]\n",
    "            path = path + [END_REL]\n",
    "            prefix_list = []\n",
    "            for rel in path:\n",
    "                prefix = \",\".join(prefix_list)\n",
    "                if prefix not in pos_rels:\n",
    "                    pos_rels[prefix] = set()\n",
    "                pos_rels[prefix].add(rel)\n",
    "                prefix_list.append(rel)\n",
    "                \n",
    "        for path_json_obj in path_and_score_list:\n",
    "            try:\n",
    "                data = generate_data_list(path_json_obj, json_obj, pos_rels, kg)\n",
    "            except FunctionTimedOut:\n",
    "                continue\n",
    "            if data is not None:\n",
    "                new_data_list.extend(data)\n",
    "\n",
    "    print(\"timeout_count:\", timeout_count)\n",
    "    \n",
    "    new_data_list = np.array(new_data_list)\n",
    "    df = pd.DataFrame(new_data_list)\n",
    "    df.to_csv(dump_data_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    load_webqsp()\n",
    "    run_search_to_get_path()\n",
    "    run_score_path()\n",
    "    run_negative_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 125/3351 [21:07<9:05:07, 10.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[1;32m      2\u001b[0m     load_webqsp()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mrun_search_to_get_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     run_score_path()\n\u001b[1;32m      5\u001b[0m     run_negative_sampling()\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mrun_search_to_get_path\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataset):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m         paths \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m FunctionTimedOut:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eric_env/lib/python3.9/site-packages/func_timeout/dafunc.py:185\u001b[0m, in \u001b[0;36mfunc_set_timeout.<locals>._function_decorator.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_function_decorator\u001b[39m(func):\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wraps(func)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs : \u001b[43mfunc_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefaultTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/eric_env/lib/python3.9/site-packages/func_timeout/dafunc.py:86\u001b[0m, in \u001b[0;36mfunc_timeout\u001b[0;34m(timeout, func, args, kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     85\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 86\u001b[0m \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m stopException \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thread\u001b[38;5;241m.\u001b[39mis_alive():\n",
      "File \u001b[0;32m~/miniconda3/envs/eric_env/lib/python3.9/threading.py:1064\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eric_env/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "                PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "                PREFIX ns: <http://rdf.freebase.com/ns/>\n",
      "        \n",
      "            select distinct ?r1 where {\n",
      "                ns:m.05v8c ?r1_ ns:m.011_7j . \n",
      "                FILTER regex(?r1_, \"http://rdf.freebase.com/ns/\")\n",
      "                bind(strafter(str(?r1_),str(ns:)) as ?r1)\n",
      "            }\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
